{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-ca5d62b38a32>, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-1-ca5d62b38a32>\"\u001b[1;36m, line \u001b[1;32m4\u001b[0m\n\u001b[1;33m    1. Hashing Task!\u001b[0m\n\u001b[1;37m             ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import random as rnd\n",
    "#1. Hashing Task!\n",
    "#Bloom Filter class\n",
    "#In this first section of the task we are going to write a Bloom Filter class, i.e. a class representing and implementing everything a bloom filter should be and should do. This will be useful for a more structured as well as a more organized code.\n",
    "\n",
    "#This bloom filter class will have two instance attributes: Bloom_Filter.array and Bloom_Filter.hash_functions. The first will contain the array representing the bloom filter, the second is a list of hash functions which will be used to insert and search elements on the data structure.\n",
    "\n",
    "#To represent Bloom_Filter.array we're going to use a numpy array instead of a simple Python list. That's because we'll work with very large arrays and numpy is a lot more efficient both in terms of memory and in terms of computational efficiency.\n",
    "\n",
    "#The initializer of the class will take two parameters: the size size of the array representing the bloom filter and the list of hash functions hash_function which will be used to work with the data structure. Both these properties are private and immutable so it will not be possible to change the behaviour of a bloom filter once it is initialized with our class.\n",
    "\n",
    "#In the end the class will have two instance methods: Bloom_Filter.insert(element) and Bloom_Filter.check(element) for inserting and searching items on the bloom filter.\n",
    "\n",
    "#In [71]:\n",
    "# This class is going to represent a bloom filter, so that we can organize all the implementation and methods\n",
    "# of the data structure in a single class.\n",
    "class Bloom_Filter:\n",
    "    \n",
    "    # To the constructor we're going to pass the size of the array representing the bloom filter\n",
    "    # and the list of hash functions that will be used for our methods\n",
    "    def _init_(self, size, hash_functions):\n",
    "        self._array = np.empty(size, dtype = bool)\n",
    "        self._hash_functions = hash_functions\n",
    "    \n",
    "    # This function is for adding elements to the bloom filter\n",
    "    def insert(self, element):\n",
    "        for function in self._hash_functions:\n",
    "            self._array[function(element)] = True\n",
    "            \n",
    "    # This function is for checking if an element is possibly on the bloom filter or definitely not in it.\n",
    "    # It returns True if the element is possibly on it, False if it's definetely not on it.\n",
    "    def check(self, element):\n",
    "        for function in self._hash_functions:\n",
    "            if(not self._array[function(element)]):\n",
    "                return(False)\n",
    "        return(True)\n",
    "#Choosing parameters\n",
    "#Since we are going to use a bloom filter for our task, one of the first things we have to do is choosing the size $m$ of the array representing it. We know the folllowing approximate formula to get a reasonable value of $m$ given an error tolerance $p$ as well as the size $n$ of the elements we are going to insert on the set:$$\n",
    "    m = -\\frac{n \\ln{p}}{(\\ln{2})^{2}}\n",
    "$$.\n",
    "\n",
    "#For the error tolerance $p$ we are going to choose the value $0.01$ so that we'll have only a $1\\%$ rate of false positives.\n",
    "\n",
    "#To find $n$ we are going to inspect the data we are given. Obviously, since the purpose of the task is to not save all the given passwords in memory, we are going to count the number of passwords without saving all of them in memory. To do this we are just going to open our file of passwords and count how many lines it has (since we already know there's a password per line) by using a counter.\n",
    "\n",
    "#In [3]:\n",
    "passwords = open(\"passwords1.txt\", \"r\")\n",
    "# It's worth noting that when Python opens a file it's not going to save it in memory\n",
    "# so we are not cheating on our task by just opening the file if we don't read it all at once\n",
    "\n",
    "counter = 0\n",
    "while(passwords.readline()):\n",
    "    counter = counter + 1\n",
    "passwords.close()\n",
    "print(counter)\n",
    "100000000\n",
    "We have found out thar our list consists of $100$ milions passwords. Knowing this we can finally compute $m$ with the formula given above and get$$\n",
    "    m = 958505838\n",
    "$$.\n",
    "\n",
    "Having established $p$ and $m$ we only need to find the number of hash functions $k$ we're going to code and use for our task. Again, there's a formula with which we'll get $k$ easily:$$\n",
    "    k = \\frac{m}{n}\\ln{2}\n",
    "$$and we get$$\n",
    "    k = 6.64\n",
    "$$so we're going to use seven hash functions.\n",
    "\n",
    "Hash functions\n",
    "This is one critical section. Here we are going to code the hash functions we'll use for our bloom filter. As we have established on the previous sections we are going to code seven of these functions and we'll try to make them as good as we can, meaning that not only they need to be good hash functions, but also independent from each other so to increase the efficiency of our data structure.\n",
    "\n",
    "A good hash function really depends on the distribution of the elements it's going to convert. That's why, before coding our functions, we're going to insepect our passwords data hoping to find some information about its underlying distribution.\n",
    "\n",
    "We already know that a password is a string of 20 characters, so we want to find what characters are possibly contained in a password as well as if some characters tend to appear more often than others.\n",
    "\n",
    "In [4]:\n",
    "passwords = open(\"passwords1.txt\", \"r\")\n",
    "\n",
    "# We're going to save the minimum as well the maximum possible character in our file (characters are ordered by their ASCII code)\n",
    "minimum = 102\n",
    "maximum = 102\n",
    "\n",
    "# We're going to look at only the first 1'000'000 entries of the file so to speed up the process\n",
    "# implicitly assuming that the underlying distribution is homogenous throughout the file\n",
    "for _ in range(1000000):\n",
    "    string = passwords.readline()\n",
    "    for character in string[:19]: # It's important we get rid of the last character, which is always a \"\\n\"\n",
    "        if(ord(character) < minimum):\n",
    "            minimum = ord(character)\n",
    "        if(ord(character) > maximum):\n",
    "            maximum = ord(character)\n",
    "\n",
    "print(minimum, chr(minimum))\n",
    "print(maximum, chr(maximum))\n",
    "passwords.close()\n",
    "33 !\n",
    "122 z\n",
    "#We see that every password can contain characters ranging from \"!\" to \"z\".\n",
    "\n",
    "#We now want to know if any of this character appears more often than the others.\n",
    "\n",
    "#In [4]:\n",
    "passwords = open(\"passwords1.txt\", \"r\")\n",
    "\n",
    "# Here we're going to save how many times a character appears on the file, at position i will be the number of times\n",
    "# chr(i + 33) appeared\n",
    "counter = [0] * (122 - 33 + 1)\n",
    "\n",
    "# Again we're just looking at the first 1'000'000 to speed up the process\n",
    "for _ in range(1000000):\n",
    "    string = passwords.readline()\n",
    "    for character in string[:19]:\n",
    "        counter[ord(character) - 33] += 1\n",
    "\n",
    "passwords.close()\n",
    "In [5]:\n",
    "counter\n",
    "Out[5]:\n",
    "[226536,\n",
    " 226375,\n",
    " 226357,\n",
    " 226105,\n",
    " 226044,\n",
    " 226000,\n",
    " 226268,\n",
    " 226404,\n",
    " 225767,\n",
    " 225890,\n",
    " 225885,\n",
    " 226388,\n",
    " 226831,\n",
    " 225541,\n",
    " 225986,\n",
    " 226636,\n",
    " 225616,\n",
    " 227077,\n",
    " 226304,\n",
    " 227385,\n",
    " 226377,\n",
    " 225768,\n",
    " 226336,\n",
    " 226474,\n",
    " 226330,\n",
    " 226024,\n",
    " 226416,\n",
    " 226617,\n",
    " 226811,\n",
    " 226216,\n",
    " 226053,\n",
    " 226097,\n",
    " 225798,\n",
    " 226659,\n",
    " 225852,\n",
    " 226279,\n",
    " 226296,\n",
    " 226135,\n",
    " 226755,\n",
    " 226109,\n",
    " 226002,\n",
    " 225869,\n",
    " 226628,\n",
    " 225940,\n",
    " 226091,\n",
    " 226075,\n",
    " 225593,\n",
    " 225928,\n",
    " 225867,\n",
    " 226701,\n",
    " 225958,\n",
    " 226349,\n",
    " 226193,\n",
    " 226762,\n",
    " 225935,\n",
    " 226347,\n",
    " 226287,\n",
    " 226075,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 0,\n",
    " 226032,\n",
    " 225611,\n",
    " 226913,\n",
    " 226309,\n",
    " 225951,\n",
    " 226027,\n",
    " 225492,\n",
    " 226486,\n",
    " 225835,\n",
    " 225963,\n",
    " 226979,\n",
    " 226746,\n",
    " 225956,\n",
    " 226440,\n",
    " 226128,\n",
    " 225894,\n",
    " 225349,\n",
    " 225799,\n",
    " 226374,\n",
    " 226345,\n",
    " 225788,\n",
    " 225929,\n",
    " 225759,\n",
    " 225880,\n",
    " 226971,\n",
    " 225647]\n",
    "Looking at this list we get two useful informations. First of all we see that characters whose ASCII code ranges from 91 ton 96 never appear on the file. Moreover we note that the remaining character appear, approximately, with the same probability in a password.\n",
    "\n",
    "Based on all these informations we're going to make the following assumption: every password in the file is a randome string where every character is independently drawn in the set of characters whose ASCII code ranges from 33 to 122, excluding those one whose code ranges from 91 to 96.\n",
    "\n",
    "The efficiency of the algorithms we're going to use will tell us if this has been a reasonable assumption, but for now we stick to it.\n",
    "\n",
    "First hash function\n",
    "For our first hash function we're going to read every password as a number in base $84$ (the numbers of possible characters appearing in a password) and then take the remainder of the division between this number and $958505838$. This should be a good hash function for the following reason: based on our previous assumption, a password is a number drawn uniformly from $0$ to $84^{20} - 1$. Given a number $x$ such that $0 \\leq x &lt; 958505838$ we get that the probability $p(x)$ of getting $x$ from a random password with this function is approximately $\\frac{1}{958505838}$.\n",
    "\n",
    "For now we code a function to get the base 10 value of a character which will be needed to treat every string as a number in base 84.\n",
    "\n",
    "In [72]:\n",
    "def get_base_10(character):\n",
    "    value = ord(character)\n",
    "    \n",
    "    # We remember that values ranging from 91 to 96 do not appear\n",
    "    if(value < 91):\n",
    "        return(value - 33)\n",
    "    else:\n",
    "        return(value - 39)\n",
    "We are now going to code our first hash function hash_1(string). Given a string $x_1x_2x_3\\cdots x_{20}$ this will represent the number$$\n",
    "    x_{1} + x_{2} 84 + x_{3} 84^{2} + \\cdots + x_{20} 84^{19}\n",
    "$$and what we need is to take the ramainder between this number and $958505838$. To do this we're going to use Horner's method to compute a polynomial, always working $\\text{mod } 958505838$.\n",
    "\n",
    "In [73]:\n",
    "def hash_1(string):\n",
    "    value  = 0\n",
    "    for index in range(len(string) - 1, -1, -1):\n",
    "        value = (84 * value + get_base_10(string[index])) % 958505838\n",
    "    return(value)\n",
    "Other hash functions\n",
    "For the other six hash functions we're going to change a little our first function hash_1. In fact we're going to use exactly this function, but instead of using it in our normal string, we're reordering the characters of the string and apply the function to the result.\n",
    "\n",
    "To start we're going to define a function to change, in some way, the order of characters in a string. We can't change this order casually, since every hash function must be deterministic. What we do is rotating the characters in a string by a step $s$ and this should be enough to obtain a totally different value from the one resulting from hash_1.\n",
    "\n",
    "In [74]:\n",
    "def rotate_string(string, step):\n",
    "    return(string[step:] + string[:step])\n",
    "We can now define our other six functions and since they will be all similar we're going to define an high order function which will take as parameters one hash function (in our case hash_1) and a number $k$ and will return our $k$-th hash function.\n",
    "\n",
    "In [75]:\n",
    "def hash_function(first_hash_function, k):\n",
    "    \n",
    "    # Our k-th hash function will just apply hash_1 to the string rotated by k steps\n",
    "    return(lambda x : hash_1(rotate_string(x, k - 1)))\n",
    "We now save all our hash functions in a list so that we can pass it as a parameter when we're going to inizialize our Bloom_Filter class.\n",
    "\n",
    "In [76]:\n",
    "hash_functions = [hash_function(hash_1, index + 1) for index in range(7)]\n",
    "As we have already said at the start of the sections, even if our functions seem to be good hash functions by themselves, it's not guaranteed that they are good hash functions for our bloom filter implementation. It could happen that their result are highly dependent, thus decreasing the efficiency of our structure.\n",
    "\n",
    "We should find a way to test the independence of our hash functions, but unfortunately it's actually impossible that they are exactly independent.\n",
    "\n",
    "In fact let's suppose for a moment that they are independent. Then taken any seven values $v = (v_{1},\\dots, v_{7})$ from $\\{0, 1, \\dots, 958505838\\}$ the probability $p_{(v_{1},\\dots, v_{7})}$ of getting $v$ after drawing a random string of $20$ characters subject to our restriction would be$$\n",
    "    p_{v_{1}}p_{v_{2}}\\cdots p_{v_{7}} = \\left(\\frac{1}{958505838}\\right)^{7} \\approx \\frac{1}{7 \\cdot 10^{62}}\n",
    "$$at the same time it would be$$\n",
    "    p_{v} = \\sum_{s \\in S_{v}} p(s)\n",
    "$$where $S_{v}$ is the set of strings giving $v$ after applying the seven hash functions. Given $s$ we have$$\n",
    "    p(s) = \\left(\\frac{1}{84}\\right)^{20} \\approx \\frac{1}{3 \\cdot 10^{38}}\n",
    "$$but this means, by the second equation,$$\n",
    "    p_{(v_{1},\\dots, v_{7})} \\geq \\frac{1}{3 \\cdot 10^{38}}\n",
    "$$which is totally in contrast with our first equation.\n",
    "\n",
    "Since we have proved that it's impossible to find seven hash functions exactly independent for our purpose, we're going to use the seven functions we have already coded, hoping they will be enough \"random\" for a good implementation of the bloom filter. The running time of our algorithm will help us finding out if everything works enough fine later.\n",
    "\n",
    "The algorithm\n",
    "We can finally code the algorithm for solving our task. We're going to code the solution in a function for a more organized and structured code.\n",
    "\n",
    "In [78]:\n",
    "# The function takes as parameters the name of the file containing the first data set, the name of the file \n",
    "# containing the second data set, the size m of the array used to represen the bloom filter and\n",
    "# the list of hash functions used by the bloom filter\n",
    "\n",
    "# The function returns the number of strings from the second data set that are possibly contained in the first data set\n",
    "# and the execution time for finding this number\n",
    "def task(first_data_set, second_data_set, m, hash_functions):\n",
    "    \n",
    "    # We initialize our bloom filter\n",
    "    bloom_filter = Bloom_Filter(m, hash_functions)\n",
    "    \n",
    "    # We add every string in the first data set to the bloom filter\n",
    "    strings = open(first_data_set, \"r\")\n",
    "    start = time.time()\n",
    "    while(True):\n",
    "        string = strings.readline()\n",
    "        if(string == \"\"):\n",
    "            break\n",
    "        string = string[:len(string) - 1] # We need to get rid of the \"\\n\" at the end\n",
    "        bloom_filter.insert(string)\n",
    "    strings.close()\n",
    "    \n",
    "    # We now check how many strings from the second data set are probably on the first data set\n",
    "    # and we also create a list containing this possibly duplicates\n",
    "    strings = open(second_data_set, \"r\")\n",
    "    possibly_duplicates = []\n",
    "    while(True):\n",
    "        string = strings.readline()\n",
    "        if(string == \"\"):\n",
    "            break\n",
    "        string = string[:len(string) - 1]\n",
    "        if(bloom_filter.check(string)):\n",
    "            possibly_duplicates.append(string)\n",
    "    end = time.time()\n",
    "    strings.close()\n",
    "    \n",
    "    return((possibly_duplicates, end - start))\n",
    "We are going to run the function and thus completing our task.\n",
    "\n",
    "In [80]:\n",
    "result = task(\"passwords1.txt\", \"passwords2.txt\", 958505838, hash_functions)\n",
    "\n",
    "# We print the asked results\n",
    "print('Number of hash functions used: ', len(hash_functions))\n",
    "print('Number of possibly duplicates: ', len(result[0]))\n",
    "print('Probability of false positives: 0.01')\n",
    "print('Execution time: ', result[1])\n",
    "Bonus\n",
    "Here we are going to count the exact number of false positives we got on the previous task. To do so we're going to use a hashing table and the hash function hash_1 in the following way.\n",
    "\n",
    "We create an array hash_duplicates of size $958505838$ where every entry is a null value or a list of strings based on our list of possibly duplicates. In fact, for every string s in our list, we compute hash_1(s) and add that string to the hash_1(s)-th position of our first array.\n",
    "\n",
    "We then iterate over the first data set of passwords and for every password p we check if p is in the hash_1(p)-th position of our array. If the answer is yes we're going to remove that string from hash_duplicates[hash_1(p)].\n",
    "\n",
    "In the end we iterate through hash_duplicates for reamining strings, those will be our false positives.\n",
    "\n",
    "Assuming that our hash function behaves well (and we hope so based on our discussions in the previous sections) all this process should take an amount of time linear in the size $n = 100000000$ of the first data set, which seems reasonable.\n",
    "\n",
    "So let's start by creating our array hash_duplicates.\n",
    "\n",
    "In [44]:\n",
    "# With this function we create an hash table of size size and with hash function hash_function and immediately\n",
    "# populate it with some data\n",
    "def hash_table(size, data, hash_function):\n",
    "    hash_array = np.empty(size, dtype = list)\n",
    "    for element in data:\n",
    "        index = hash_function(element)\n",
    "        if(hash_array[index] is None):\n",
    "            hash_array[index] = [element]\n",
    "        else:\n",
    "            hash_array[index].append(element)\n",
    "    return(hash_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
